{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "import torch\n",
    "import imgviz\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def save_colored_mask(save_path, mask_pil):\n",
    "    \"\"\"保存调色板彩色图\"\"\"\n",
    "    lbl_pil = mask_pil.convert('P')\n",
    "    # lbl_pil = Image.fromarray(mask.astype(np.uint8), mode='P')\n",
    "    colormap = imgviz.label_colormap(80)\n",
    "    # colormap[0] = [169, 169, 169]  # 灰色的RGB值\n",
    "    colormap[0] = [0, 0, 0]  # 灰色的RGB值\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    lbl_pil.save(save_path)\n",
    "\n",
    "def save_bbox_img(colored_res_path, bboxes, size=512, name=\"bbox.png\"):\n",
    "    scale_fct = torch.tensor([size, size, size, size]).to(device)\n",
    "    bboxes = bboxes * scale_fct\n",
    "    out_image = Image.new('L', (size, size), 0)\n",
    "    draw = ImageDraw.Draw(out_image)\n",
    "    for n, b in enumerate(bboxes):\n",
    "        draw.rectangle(((b[0], b[1]), (b[2], b[3])),\n",
    "                       fill=None, outline=n+1, width=5)\n",
    "    save_colored_mask(os.path.join(colored_res_path, name), out_image)\n",
    "    plt.imshow(out_image)\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "    plt.show()\n",
    "    return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layout = [[0.46, 0.29, 0.90, 0.82], [0.02, 0.28, 0.32, 0.85], [0.34, 0.72, 0.60, 0.94]] # [x1, y1, x2, y2]\n",
    "bboxes = torch.tensor(layout).to(device)\n",
    "out_image = save_bbox_img(\"results\", bboxes, size=512, name=\"bbox.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "import argparse\n",
    "import torch\n",
    "from boxnet_models import build_model, add_boxnet_args\n",
    "from utils.utils import save_colored_mask, save_bbox_img, save_img_with_box\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "boxnet_path = \"./ckpt/boxnet.pth\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = add_boxnet_args(parser)\n",
    "args = parser.parse_args()\n",
    "if boxnet_path is not None:\n",
    "    args.no_class = True\n",
    "    boxnet, _, _ = build_model(args)\n",
    "    boxnet.load_state_dict(torch.load(boxnet_path))\n",
    "    boxnet = boxnet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/conda/envs/ICML/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 11.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, EulerDiscreteScheduler\n",
    "from torchvision.utils import save_image, make_grid\n",
    "model_id = \"/data/zsz/ssh/models/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/\"\n",
    "# model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "# Use the DPMSolverMultistepScheduler (DPM-Solver++) scheduler here instead\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=\"/data/zsz/models/storage_file/models/\",resume_download=True)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed = int(torch.rand((1,)) * 100000)\n",
    "generator = torch.Generator(device).manual_seed(seed)\n",
    "def save_image_cuda(x, path, nrow=1, normalize=True, value_range=(-1., 1.)):\n",
    "    img = make_grid(x, nrow=nrow, normalize=normalize)\n",
    "    save_image(img, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A solitary lighthouse perched on rugged cliffs illuminates the waves below. The horizon is painted in hues of pink and gold.\"\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"astronaut_rides_horse.png\")\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from diffusers.utils import PIL_INTERPOLATION\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL_INTERPOLATION[\"lanczos\"])\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "init_latents = Image.open('./4.jpg').convert('RGB')\n",
    "init_latents = preprocess_image(init_latents)\n",
    "shape = init_latents.shape\n",
    "init_latents = init_latents.to(device)\n",
    "noise = torch.randn(shape, generator=generator, device=device) # if len(noise_preds) == 0 else noise_preds[time]\n",
    "pipe.scheduler.set_timesteps(1000)\n",
    "timesteps = pipe.scheduler.timesteps.to(device)\n",
    "time = 960\n",
    "t = timesteps[time - 1] * torch.ones(init_latents.shape[0]).to(device)\n",
    "t = t.long()\n",
    "latents = pipe.scheduler.add_noise(init_latents, noise, t)\n",
    "save_image_cuda(latents, \"latents.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config == '__is_first_stage__':\n",
    "            return None\n",
    "        elif config == \"__is_unconditional__\":\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "\n",
    "def get_obj_from_str(string, reload=False):\n",
    "    module, cls = string.rsplit(\".\", 1)\n",
    "    if reload:\n",
    "        module_imp = importlib.import_module(module)\n",
    "        importlib.reload(module_imp)\n",
    "    return getattr(importlib.import_module(module, package=None), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "from PIL import Image\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load sample image\n",
    "raw_image = Image.open(\"./1.jpg\").convert(\"RGB\")\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)\n",
    "caption = \"an elephat is left to a cat\"\n",
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "text_input = txt_processors[\"eval\"](caption)\n",
    "sample = {\"image\": image, \"text_input\": [text_input]}\n",
    "\n",
    "features_multimodal = model.extract_features(sample)\n",
    "print(features_multimodal.multimodal_embeds.shape)\n",
    "# torch.Size([1, 12, 768]), use features_multimodal[:,0,:] for multimodal classification tasks\n",
    "\n",
    "features_image = model.extract_features(sample, mode=\"image\")\n",
    "features_text = model.extract_features(sample, mode=\"text\")\n",
    "print(features_image.image_embeds.shape)\n",
    "# torch.Size([1, 197, 768])\n",
    "print(features_text.text_embeds.shape)\n",
    "# torch.Size([1, 12, 768])\n",
    "\n",
    "# low-dimensional projected features\n",
    "print(features_image.image_embeds_proj.shape)\n",
    "# torch.Size([1, 197, 256])\n",
    "print(features_text.text_embeds_proj.shape)\n",
    "# torch.Size([1, 12, 256])\n",
    "similarity = features_image.image_embeds_proj[:,0,:] @ features_text.text_embeds_proj[:,0,:].t()\n",
    "print(similarity)\n",
    "# tensor([[0.2622]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a white plate topped with a cut in half sandwich next to a bowl of soup']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" \n",
    "import torch\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from PIL import Image\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "raw_image = Image.open(\"./4.jpg\").convert(\"RGB\")\n",
    "# loads BLIP caption base model, with finetuned checkpoints on MSCOCO captioning dataset.\n",
    "# this also loads the associated image processors\n",
    "model, vis_processors, _ = load_model_and_preprocess(name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device)\n",
    "# preprocess the image\n",
    "# vis_processors stores image transforms for \"train\" and \"eval\" (validation / testing / inference)\n",
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "# generate caption\n",
    "model.generate({\"image\": image})\n",
    "# ['a large fountain spewing water into the air']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boxnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
